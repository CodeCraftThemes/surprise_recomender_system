import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def show():
    st.title("üìä Evaluaci√≥n de Modelos")
    
    # Sidebar con controles
    with st.sidebar:
        st.header("Configuraci√≥n")
        dataset = st.selectbox("Dataset:", ["ml-100k"], key="eval_dataset")
        
        # Diccionario de funciones de entrenamiento
        model_functions = {
            "KNNBasic": get_knn_train_function("KNNBasic"),
            "KNNWithMeans": get_knn_train_function("KNNWithMeans"),
            "SVD": get_svd_train_function("SVD"),
            "SVDpp": get_svd_train_function("SVDpp"),
            "NMF": get_nmf_train_function(),
            "SlopeOne": get_slope_one_train_function()
        }
        
        selected_models = st.multiselect(
            "Modelos a evaluar:",
            list(model_functions.keys()),
            default=list(model_functions.keys())
        )
    
    # Vista principal
    if st.button("‚úÖ Ejecutar Evaluaci√≥n Comparativa"):
        if not selected_models:
            st.warning("‚ö†Ô∏è Por favor selecciona al menos un modelo")
            return
            
        with st.spinner(f"Evaluando {len(selected_models)} modelos. Por favor espere..."):
            from utils.evaluator import compare_models
            
            # Cargar solo los modelos seleccionados
            models_to_evaluate = {name: model_functions[name] for name in selected_models}
            
            try:
                results = compare_models(models_to_evaluate, dataset)
                display_metrics(results)
            except Exception as e:
                st.error(f"‚ùå Error durante la evaluaci√≥n: {str(e)}")

def get_knn_train_function(model_type):
    """Devuelve funci√≥n de entrenamiento para modelos KNN"""
    from models.knn.knn_train import train_knn_model
    return lambda: train_knn_model(model_type=model_type)

def get_svd_train_function(model_type):
    """Devuelve funci√≥n de entrenamiento para modelos SVD"""
    from models.svd.svd_train import train_svd_model
    return lambda: train_svd_model(model_type=model_type)

def get_nmf_train_function():
    """Devuelve funci√≥n de entrenamiento para NMF"""
    from models.nmf.nmf_train import train_nmf_model
    return train_nmf_model

def get_slope_one_train_function():
    """Devuelve funci√≥n de entrenamiento para SlopeOne"""
    from models.slope_one.slope_one_train import train_slope_one
    return train_slope_one

def display_metrics(results_df):
    """Muestra m√©tricas con gr√°ficos y an√°lisis"""
    st.subheader("Resultados de Evaluaci√≥n")
    
    # M√©tricas principales
    metrics = [
        ('RMSE', 'Error Cuadr√°tico Medio (menor es mejor)'),
        ('MAE', 'Error Absoluto Medio (menor es mejor)'),
        ('Fit Time', 'Tiempo de Entrenamiento (segundos)'),
        ('Test Time', 'Tiempo por Predicci√≥n (ms)')
    ]
    
    for metric, description in metrics:
        if metric in results_df.columns:
            # Gr√°fico de barras
            st.markdown(f"### {description}")
            fig, ax = plt.subplots(figsize=(10, 4))
            results_df[metric].sort_values().plot(
                kind='bar', 
                ax=ax,
                color=sns.color_palette("viridis", len(results_df))
            )
            plt.xticks(rotation=45)
            ax.set_xlabel("Modelos")
            ax.set_ylabel(metric)
            st.pyplot(fig)
            
            # An√°lisis interpretativo
            is_lower_better = metric in ['RMSE', 'MAE', 'Fit Time', 'Test Time']
            best_value = results_df[metric].min() if is_lower_better else results_df[metric].max()
            best_model = results_df[metric].idxmin() if is_lower_better else results_df[metric].idxmax()
            
            st.markdown(f"""
            **Mejor modelo**: `{best_model}` ({best_value:.4f})
            
            **Interpretaci√≥n**:
            {get_metric_interpretation(metric, best_model, best_value)}
            """)
            
            st.divider()
    
    # Resumen ejecutivo
    st.subheader("üìå Resumen Comparativo")
    st.dataframe(
        results_df.style.highlight_min(
            subset=['RMSE', 'MAE', 'Fit Time', 'Test Time'],
            color='lightgreen'
        )
    )

def get_metric_interpretation(metric, best_model, best_value):
    """Provee explicaciones t√©cnicas contextualizadas"""
    interpretations = {
        'RMSE': (
            f"El modelo {best_model} tiene el menor error cuadr√°tico medio ({best_value:.4f}). "
            "Valores t√≠picos buenos para ml-100k est√°n entre 0.85-1.00. "
            "Diferencias >0.05 entre modelos son consideradas significativas."
        ),
        'MAE': (
            f"El modelo {best_model} tiene el menor error absoluto ({best_value:.4f}). "
            "Esta m√©trica es m√°s f√°cil de interpretar directamente que RMSE. "
            "Por ejemplo, un MAE de 0.75 significa que en promedio las predicciones "
            "se desv√≠an ¬±0.75 estrellas del rating real."
        ),
        'Fit Time': (
            f"El modelo {best_model} fue el m√°s r√°pido en entrenarse ({best_value:.2f} segundos). "
            "KNN y SVDpp suelen ser los m√°s lentos, mientras que SlopeOne y CoClustering "
            "son generalmente los m√°s r√°pidos."
        ),
        'Test Time': (
            f"El modelo {best_model} tiene el menor tiempo por predicci√≥n ({best_value:.4f} ms). "
            "Importante para sistemas que requieren recomendaciones en tiempo real."
        )
    }
    return interpretations.get(metric, "M√©trica sin interpretaci√≥n espec√≠fica.")

if __name__ == "__main__":
    show()